{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "- Implement a basic Tf-Idf search.\n",
    "\n",
    "\n",
    "#### Workflow\n",
    "\n",
    "- Load all relevant Python libraries and a spaCy language model.\n",
    "\n",
    "- Access the tokenized text in your new dataset from the previous milestone. Each document dictionary should now include a new key-value pair with the lemmatized text of the articles.\n",
    "\n",
    "- Create a corpus vocabulary. It should simply be a list of unique tokens in the provided set of documents. Count how many times each unique token appears in the corpus, you will need these counts for the next step.\n",
    "\n",
    "- Calculate Tf-Idf vectors for every article in the dataset and add these vectors to the article dictionaries. You should end up the same list of dictionaries as before, but with a new key-value pair containing Tf-Idf vectors:\n",
    "\n",
    "- title: Title of the Wikipedia article the text is taken from.\n",
    "- text: Wikipedia article text. (In this dataset we included only the summary.)\n",
    "- tokenized_text: Tokenized Wikipedia article text.\n",
    "- url: Link to the Wikipedia article.\n",
    "- tf_idfs: Tf_Idf vector.\n",
    "\n",
    "Now we can try to search our list of dictionaries using this Tf-Idf field using existing tools for similarity. We suggest you use scikit-learn library and its cosine_similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Spacy Language model\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/result.json\", mode = \"r\") as file:\n",
    "    file_obj = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus vocabulary. It should simply be a list of unique tokens in the provided\n",
    "# set of documents. Count how many times each unique token appears in the corpus, \n",
    "# you will need these counts for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(docs):\n",
    "    \"\"\"\n",
    "    Create a corpus itering all documents.\n",
    "    \n",
    "    :return corpus: Return a list of strings.\n",
    "    \"\"\"\n",
    "    corpus = [doc[\"tokenized_text\"]  for doc in docs]\n",
    "    corpus = list(chain(*corpus))\n",
    "    return   list(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['social',\n",
       " 'driver',\n",
       " 'dropping',\n",
       " 'rafe',\n",
       " 'donor',\n",
       " 'previous',\n",
       " 'fatigue',\n",
       " 'import',\n",
       " 'latin',\n",
       " 'say']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = create_corpus(file_obj)\n",
    "### Contiene las palabras de todos los documentos y su frecuecnia en todos los documentos.\n",
    "corpus[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_document(corpus, docs):\n",
    "    \"\"\"\n",
    "    Conteo de aparacion de una palabra en todos los documentos.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    dict_count = {}\n",
    "    \n",
    "    for item in corpus:\n",
    "        ### Conteo de veces que la palabra en el corpus\n",
    "        ### Aparece en todos los documentos\n",
    "        for doc in docs:\n",
    "            if item in doc[\"tokenized_text\"]:\n",
    "                count+=1\n",
    "        \n",
    "        ## Finalizing\n",
    "        dict_count[item] = count\n",
    "        count = 0\n",
    "        \n",
    "    return dict_count\n",
    "\n",
    "\n",
    "def compute_tf(tokens):\n",
    "    \"\"\"\n",
    "    Calculate tf for each list of tokens.\n",
    "    \"\"\"\n",
    "    count_obj = Counter(tokens)\n",
    "    total_terms = len(tokens)\n",
    "    \n",
    "    dict_count = {}\n",
    "    \n",
    "    for key, value in count_obj.items():\n",
    "        dict_count[key] = value\n",
    "   \n",
    "    return count_obj\n",
    "\n",
    "    \n",
    "def compute_idf(docs, dict_count):\n",
    "    \"\"\"\n",
    "    log(N/C)\n",
    "    \n",
    "    - N:  Total number of documents.\n",
    "    - nw: Number of documents containing the word w.\n",
    "    \"\"\"\n",
    "    N = len(docs)\n",
    "    idf_dict = {}\n",
    "    \n",
    "    for key, count in dict_count.items():\n",
    "        idf_dict[key] = np.log(N/count)\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/result.json\", mode = \"rb\") as file:\n",
    "    docs = json.load(file)\n",
    "    corpus = create_corpus(docs)\n",
    "    \n",
    "    ### Idf\n",
    "    dict_count_document = count_word_document(corpus, docs)\n",
    "    idf = compute_idf(docs, dict_count_document)\n",
    "    \n",
    "    ### Term Frequency\n",
    "    for index, doc in enumerate(docs):\n",
    "        dict_tf = compute_tf(doc[\"tokenized_text\"])\n",
    "\n",
    "        ### For each term in a document, we calculate the TF-IDF\n",
    "        tf_idf_list = []\n",
    "        \n",
    "        for term in corpus:\n",
    "            tf_idf_list.append((dict_tf[term]/len(doc[\"tokenized_text\"])) * idf[term])\n",
    "            \n",
    "\n",
    "        docs[index][\"tf_idfs\"] = tf_idf_list\n",
    "        \n",
    "    ## At last the result is saved.            \n",
    "    with open('data/tfidf_result.json', 'w') as output:\n",
    "        json.dump(docs, output)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/corpus.json', 'w') as corpusfile:\n",
    "    json.dump(corpus, corpusfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search function\n",
    "- Create a search function to compute cosine similarities \n",
    "- between the document Tf-Idf vectors and the query Tf-Idf vector.\n",
    "- Save this new list of dictionaries as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(content_doc):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # doc contains the text lemmas and their attributes\n",
    "    # https://spacy.io/api/doc\n",
    "    doc = sp(content_doc.lower())\n",
    "    \n",
    "    # remove stop words\n",
    "    all_stopwords = sp.Defaults.stop_words\n",
    "    tokens_without_sw = [word for word in doc if word not  in all_stopwords]\n",
    "    \n",
    "    # remove punctuation and stop words\n",
    "    tokens_without_pct = [token for token in tokens_without_sw \n",
    "                          if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "    # Get Lemmas\n",
    "    token_lemmas = [\n",
    "                        token.lemma_ for token\n",
    "                            in tokens_without_pct\n",
    "                               if len(token.dep_.strip())>0\n",
    "                                and token.lemma_ != \"\\n\"\n",
    "                                and token.dep_\n",
    "                        ]\n",
    "    \n",
    "    return token_lemmas \n",
    "\n",
    "\n",
    "def vectorize(query, docs, corpus):\n",
    "    query_tokenized = tokenizer(query)\n",
    "    query_token_counter = Counter(query_tokenized)\n",
    "    query_vec = []\n",
    "    \n",
    "    for token in corpus:\n",
    "        tf = query_token_counter[token] / len(query_tokenized)\n",
    "        idf = np.log(len(docs) /  dict_count_document[token])\n",
    "        tfidf = tf * idf\n",
    "        query_vec.append(tfidf)\n",
    "    return query_vec\n",
    "\n",
    "def search_similarity(query, docs, corpus):\n",
    "    query_vec = vectorize(query, docs, corpus)\n",
    "    query_arr = np.array(query_vec)\n",
    "\n",
    "    rankings = []\n",
    "    for doc in docs:\n",
    "        doc_rank = {}\n",
    "        coefficient = cosine_similarity(\n",
    "                                     query_arr.reshape(1, -1), \n",
    "                                     np.array(doc[\"tf_idfs\"]).reshape(1, -1)\n",
    "                                    )[0][0]\n",
    "\n",
    "        if coefficient > 0:\n",
    "            doc_rank['title'] = doc['title']\n",
    "            doc_rank['rank'] = coefficient\n",
    "            rankings.append(doc_rank)\n",
    "\n",
    "    return sorted(rankings, key=lambda k: k['rank'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Swine influenza', 'rank': 0.3753601130402447},\n",
       " {'title': 'Spanish flu', 'rank': 0.2641577193914792},\n",
       " {'title': 'Pandemic', 'rank': 0.0828231483433225},\n",
       " {'title': 'Unified Victim Identification System',\n",
       "  'rank': 0.03771210435958485}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_file(name = 'result'):\n",
    "    \"\"\"\n",
    "    Load any file in json format, within the folder data.\n",
    "    :param name: Nmae of the file.\n",
    "    :type name: str\n",
    "    \"\"\"\n",
    "    with open(f\"data/{name}.json\", mode = \"rb\") as file:\n",
    "        file_obj = json.load(file)\n",
    "        return file_obj\n",
    "    \n",
    "###############################\n",
    "###############################\n",
    "###############################\n",
    "corpus = load_file('corpus')\n",
    "docs = load_file('tfidf_result')\n",
    "\n",
    "# query = \"highest pandemic casualties\"\n",
    "query = \"flu\"\n",
    "search_similarity(query, docs, corpus)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
